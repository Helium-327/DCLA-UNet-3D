# -*- coding: UTF-8 -*-
'''
================================================
*      CREATE ON: 2025/01/06 15:53:45
*      AUTHOR: @Junyin Xiong
*      DESCRIPTION: 
*      VERSION: v1.0
=================================================
'''

import os
import time
import torch
import torch.nn as nn
import argparse
import logging
from train_swanlab import train
from tabulate import tabulate
from torch.utils.data import DataLoader
from metrics import EvaluationMetrics
from torch.amp import GradScaler
from train_init import load_model, load_loss, load_optimizer, load_scheduler
from utils import *
from lossFunc import *
from metrics import *
from datasets.BraTS2021 import DatasetSplitter


# ç¯å¢ƒè®¾ç½®
# torch.backends.cudnn.benchmark = False         #! åŠ é€Ÿå›ºå®šè¾“å…¥/ç½‘ç»œç»“æ„çš„è®­ç»ƒï¼Œä½†éœ€é¿å…åŠ¨æ€å˜åŒ–åœºæ™¯ï¼Œå¦‚æ•°æ®å¢å¼º
# torch.backends.cudnn.deterministic = True     #! ç¡®ä¿ç»“æœå¯å¤ç°ï¼Œä½†å¯èƒ½é™ä½æ€§èƒ½å¹¶å¼•å‘å…¼å®¹æ€§é—®é¢˜

# å¤ç°æ¨¡å¼ï¼ˆæµ‹è¯•/è®ºæ–‡ï¼‰
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True

# # æ€§èƒ½æ¨¡å¼ï¼ˆé»˜è®¤æ¨èï¼‰
# torch.backends.cudnn.benchmark = True
# torch.backends.cudnn.deterministic = False

# !è°ƒè¯•å·¥å…·(ä¸ä¼šç”¨å°±ä¸ç”¨ï¼Œä¸ç„¶ä¼šåæ‚”çš„ğŸ§ï¼Œ å“¥ä»¬)
# os.environ["CUDA_LAUNCH_BLOCKING"] = '1'    #! æ£€æµ‹æ¯ä¸ªæ“ä½œçš„è¾“å…¥å’Œè¾“å‡ºå¼ é‡çš„å½¢çŠ¶
# torch.autograd.set_detect_anomaly(True)     #! æ£€æµ‹æ¢¯åº¦å¼‚å¸¸ï¼Œä½†ä¼šé™ä½æ€§èƒ½ï¼ˆâš ï¸è°¨æ…ä½¿ç”¨ï¼Œå“¥ä»¬ï¼‰

DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
RANDOM_SEED = 42
 # æ··åˆç²¾åº¦è®­ç»ƒ
MetricsGo = EvaluationMetrics()  # å®ä¾‹åŒ–è¯„ä¼°æŒ‡æ ‡ç±»
    
# å®šä¹‰é¢œè‰²ä»£ç 
class ColorCodes:
    GREY = "\033[0;37m"
    GREEN = "\033[0;32m"
    YELLOW = "\033[0;33m"
    RED = "\033[0;31m"
    BOLD_RED = "\033[1;31m"
    RESET = "\033[0m"

# åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„æ ¼å¼åŒ–å™¨
class ColoredFormatter(logging.Formatter):
    FORMATS = {
        logging.DEBUG: ColorCodes.GREY + "%(asctime)s - %(levelname)s - %(message)s" + ColorCodes.RESET,
        logging.INFO: ColorCodes.GREEN + "%(asctime)s  - %(levelname)s - %(message)s" + ColorCodes.RESET,
        logging.WARNING: ColorCodes.YELLOW + "%(asctime)s  - %(levelname)s - %(message)s" + ColorCodes.RESET,
        logging.ERROR: ColorCodes.RED + "%(asctime)s  - %(levelname)s - %(message)s" + ColorCodes.RESET,
        logging.CRITICAL: ColorCodes.BOLD_RED + "%(asctime)s  - %(levelname)s - %(message)s" + ColorCodes.RESET
    }

    def format(self, record):
        log_fmt = self.FORMATS.get(record.levelno)
        formatter = logging.Formatter(log_fmt)
        return formatter.format(record)

# è®¾ç½®æ—¥å¿—é…ç½®
def setup_colored_logging(file_name=None):
    
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)  # è®¾ç½®æœ€ä½çº§åˆ«ï¼Œè®©æ‰€æœ‰æ—¥å¿—éƒ½èƒ½è¢«å¤„ç†
    
    #  é¢œè‰²åŒ–æ—¥å¿—
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(ColoredFormatter())
    
    file_handler = logging.FileHandler(filename=file_name, encoding='utf-8')
    file_handler.setFormatter(ColoredFormatter())
    
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    
    return logger

# åœ¨ä½ çš„ä¸»è„šæœ¬å¼€å§‹æ—¶è°ƒç”¨è¿™ä¸ªå‡½æ•°
logger = setup_colored_logging(file_name='Training.log')

def log_params(params, project_name, logs_path, logger, verbose=False):
    """è®°å½•è®­ç»ƒå‚æ•°"""
    params_dict = {'Parameter': [str(p[0]) for p in list(params.items())],
                   'Value': [str(p[1]) for p in list(params.items())]}
    params_header = ["Parameter", "Value"]
    custom_logger('='*40 + '\n' + "è®­ç»ƒå‚æ•°" +'\n' + '='*40 +'\n', logs_path, log_time=True)
    custom_logger(tabulate(params_dict, headers=params_header, tablefmt="grid"), logs_path)
    if verbose:
        logger.info(f'ğŸ§  é¡¹ç›®åï¼š{project_name} \n' + \
            tabulate(params_dict, headers=params_header, tablefmt="fancy_grid"))
    

def load_data(args):
    from datasets import (
        Compose,
        RandomFlip3D,
        RandomCrop3D,
        CenterCrop3D,
        FrontGroundNormalize,
        RandomNoise3D,
        ToTensor
    )
    if args.datasets == 'BraTS2021':
        from datasets.BraTS2021 import BraTS21_3D as BraTS_Dataset
    elif args.datasets == 'BraTS2019':
        from datasets.BraTS2019 import BraTS19_3D as BraTS_Dataset
        
    """åŠ è½½æ•°æ®é›†"""
    TransMethods_train = Compose([
        RandomFlip3D(),
        RandomCrop3D((128, 128, 128)),
        FrontGroundNormalize(),
        RandomNoise3D(mean=0.0, std=(0, 0.1)),
        ToTensor()
    ])

    TransMethods_val = Compose([
        CenterCrop3D((128, 128, 128)),
        FrontGroundNormalize(),
        ToTensor(),
    ])
    
    TransMethods_test = Compose([
        CenterCrop3D((128, 128, 128)),
        FrontGroundNormalize(),
        ToTensor(),
    ])

    train_dataset = BraTS_Dataset(
        data_file=args.train_csv_path,
        transform=TransMethods_train,
        local_train=args.local,
        length=args.train_length,
    )
    
    val_dataset = BraTS_Dataset(
        data_file=args.val_csv_path,
        transform=TransMethods_val,
        local_train=args.local,
        length=args.val_length,
    )

    test_dataset = BraTS_Dataset(
        data_file=args.test_csv_path,
        transform=TransMethods_test,
        local_train=args.local,
        length=args.test_length,
    )
    
    setattr(args, 'train_length', len(train_dataset))
    logger.warning(f"è®­ç»ƒé›†å¤§å°å˜æˆï¼š{len(train_dataset)}")
    setattr(args, 'val_length', len(val_dataset))
    logger.warning(f"éªŒè¯é›†å¤§å°å˜æˆï¼š{len(val_dataset)}")
    setattr(args, 'test_length', len(test_dataset))
    logger.warning(f"æµ‹è¯•å¤§å°å˜æˆï¼š{len(test_dataset)}")
    
    train_loader = DataLoader(
        dataset=train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=True,
        persistent_workers=True  # å‡å°‘ worker åˆå§‹åŒ–æ—¶é—´
    )
    val_loader = DataLoader(
        dataset=val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True,
        persistent_workers=True  # å‡å°‘ worker åˆå§‹åŒ–æ—¶é—´
    )
    
    test_loader = DataLoader(
        dataset=test_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True,
        persistent_workers=True  # å‡å°‘ worker åˆå§‹åŒ–æ—¶é—´
    )
    return args, train_loader, val_loader, test_loader

def main(args):
    start_epoch = 0
    best_val_loss = float('inf')
    resume_tb_path = None
    scaler = GradScaler() # æ··åˆç²¾åº¦è®­ç»ƒ
    
    """------------------------------------- ç”Ÿæˆç›¸å¯¹åœ°å€ --------------------------------------------"""
    train_csv_path = os.path.join(os.path.dirname(args.data_root), 'train.csv')
    val_csv_path = os.path.join(os.path.dirname(args.data_root), 'val.csv')
    test_csv_path = os.path.join(os.path.dirname(args.data_root), 'test.csv')
    
    setattr(args, 'train_csv_path', train_csv_path)
    setattr(args,'val_csv_path', val_csv_path)
    setattr(args,'test_csv_path', test_csv_path)
    
    """------------------------------------- æ¨¡å‹å®ä¾‹åŒ–ã€åˆå§‹åŒ– --------------------------------------------"""
    # åŠ è½½æ¨¡å‹
    model = load_model(args.model_name)
    # åŠ è½½ä¼˜åŒ–å™¨
    optimizer = load_optimizer(model, args.lr, args.wd)

    # åŠ è½½è°ƒåº¦å™¨
    scheduler = load_scheduler(optimizer, args.cosine_T_max, args.cosine_eta_min)

    # åŠ è½½æŸå¤±å‡½æ•°
    loss_function = load_loss(args.loss_type)
    total_params = sum(p.numel() for p in model.parameters())
    total_params = f'{total_params/1024**2:.2f} M'
    logger.info(f"Total number of parameters: {total_params}")
    setattr(args, 'total_parms', total_params)
    model_name = model.__class__.__name__
    """------------------------------------- å®šä¹‰æˆ–è·å–è·¯å¾„ --------------------------------------------"""
    model_name_title = ('_').join([model_name, f'{get_current_date()}_lr{args.lr}_mlr{args.cosine_eta_min}_Tmax{args.cosine_T_max}_{args.epochs}_{args.early_stop_patience}'])
    if args.resume:
        resume_path = args.resume
        logger.info(f"Resuming training from {resume_path}")
        results_dir = os.path.join('/',*resume_path.split('/')[:-2])
        resume_tb_path = os.path.join(results_dir, 'tensorBoard')
        logs_dir = os.path.join(results_dir, 'logs')
        logs_file_name = [file for file in os.listdir(logs_dir) if file.endswith('.log')]
        logs_path = os.path.join(logs_dir, logs_file_name[0])
    else:
        os.makedirs(args.results_dir, exist_ok=True)
        results_dir = os.path.join(args.results_dir, model_name_title)       # TODO: æ”¹æˆç½‘ç»œå¯¹åº”çš„æ–‡ä»¶å¤¹
        os.makedirs(results_dir, exist_ok=True)
        logs_dir = os.path.join(results_dir, 'logs')
        logs_path = os.path.join(logs_dir, f'{get_current_date()}.log')
        os.makedirs(logs_dir, exist_ok=True)

    """------------------------------------- è®°å½•å½“å‰å®éªŒå†…å®¹ --------------------------------------------"""
    # exp_commit = args.commit if args.commit else input("è¯·è¾“å…¥æœ¬æ¬¡å®éªŒçš„æ›´æ”¹å†…å®¹: ")
    # write_commit_file(os.path.join(results_dir, 'commits.md'), exp_commit)


    """------------------------------------- æ–­ç‚¹ç»­ä¼  --------------------------------------------"""
    if args.resume:
        logger.info(f"Resuming training from checkpoint {args.resume}")
        model, optimizer, scaler, start_epoch, best_val_loss, scheduler = load_checkpoint(model, 
                                                                                          optimizer, 
                                                                                          scaler, 
                                                                                          args.resume, 
                                                                                          scheduler
                                                                                          ) 
        logger.info(f"Loaded checkpoint {args.resume}")
        logger.info(f"Best val loss: {best_val_loss:.4f} âœˆ epoch {start_epoch}")
        cutoff_tb_data(resume_tb_path, start_epoch)
        logger.warning(f"Refix resume tb data step {resume_tb_path} up to step {start_epoch}")

    # åˆ†å‰²æ•°æ®é›†/åˆ¤æ–­æ˜¯å¦é‡å 
    if args.data_split and args.datasets == 'BraTS2021':
        dataset_splitter = DatasetSplitter(args.data_root)
        dataset_splitter.collect_files()
        dataset_splitter.split(random_seed=RANDOM_SEED)
        dataset_splitter.save_csv(os.path.dirname(args.data_root))
    else:
        dataset_splitter = DatasetSplitter(args.data_root)
        dataset_splitter.diff_datasets(args.train_csv_path, args.val_csv_path, args.test_csv_path, logger)
        
    # åŠ è½½æ•°æ®é›†
    args, train_loader, val_loader, test_loader = load_data(args)
    
    
    # è®°å½•è®­ç»ƒé…ç½®
    log_params(vars(args), args.slb_project, logs_path, logger, verbose=True)
    """------------------------------------- è®­ç»ƒæ¨¡å‹ --------------------------------------------"""
    train(model=model, 
          Metrics=MetricsGo, 
          train_loader=train_loader,
          val_loader=val_loader, 
          test_loader=test_loader,
          scaler=scaler, 
          optimizer=optimizer,
          scheduler=scheduler,
          loss_function=loss_function,
          num_epochs=args.epochs, 
          device=DEVICE, 
          results_dir=results_dir,
          logs_path=logs_path,
          output_path=args.outputs_dir,
          start_epoch=start_epoch,
          best_val_loss=best_val_loss,
          test_csv=args.test_csv_path,
          interval=args.interval,
          save_max=args.save_max,
          early_stopping_patience=args.early_stop_patience,
          # tb ç›¸å…³å‚æ•°          
          tb=args.tb,
          # resume_tb_path=resume_tb_path
          
          # SwanLabç›¸å…³å‚æ•°pip
          slb=args.slb,
          slb_project=args.slb_project,
          slb_experiment=model_name_title,
          logger=logger,
          slb_config=args,
          )

if __name__ == '__main__':
    start_time = time.time()
    
    # è·å–å·¥ä½œè·¯å¾„
    ROOT = "/root/workspace/DCLA-UNet"                         #TODO: åˆ‡æ¢æœºå™¨éœ€è¦æ›´æ”¹
    if not os.path.exists(ROOT):
        ROOT = "/root/autodl-tmp/DCLA-UNet/"
    
    initial_parser = argparse.ArgumentParser(add_help=False)
    
    #*** å›ºå®šå‚æ•° ***#
    initial_parser.add_argument('--data_root', 
                                type=str, 
                                default=os.path.join(ROOT, "data/BraTS2019/raw"), 
                                help='Path to the DATASET ROOT'
                                )
    initial_parser.add_argument('--outputs_dir', 
                                type=str, 
                                default=os.path.join(ROOT, 'outputs'),
                                help='Path to the DATASET ROOT'
                                )
    initial_parser.add_argument('--results_dir', 
                                type=str, 
                                default=os.path.join(ROOT, 'results'),
                                help='Path to the DATASET ROOT'
                                )
    initial_parser.add_argument('--in_channel', 
                                type=int, 
                                default=4,
                                help='Name of input channel'
                                )
    initial_parser.add_argument('--out_channel', 
                                type=int, 
                                default=4,
                                help='Name of input channel / num of classes'
                                )
    #*** å¿…ä¼ å‚æ•° ***#
    initial_parser.add_argument('--config', 
                                type=str, 
                                default=None, 
                                help='Path to the configuration YAML file'
                                )
    initial_parser.add_argument('--model_name', 
                                type=str, 
                                default="UNet3D",
                                help='Name of Model'
                                )
    initial_parser.add_argument('--slb_project', 
                                type=str, 
                                default='debug',
                                help='Name of SwanLab project'
                                )
    #*** å¯é€‰å‚æ•° ***#
    initial_parser.add_argument('--resume', 
                                type=str, 
                                default=False,
                                help='Path to the Resume checkpoint'
                                )
    initial_parser.add_argument('--loss_type', 
                                type=str, 
                                default="diceloss",
                                help='Type of Loss Function'
                                )
    initial_parser.add_argument('--optimizer_type', 
                                type=str, 
                                default="adamw",
                                help='Type of optimizer'
                                )
    initial_parser.add_argument('--epochs', 
                                type=int, 
                                default=100,
                                help='Num of epochs'
                                )
    initial_parser.add_argument('--batch_size', 
                                type=int, 
                                default=1,
                                help='Num of batch size'
                                )
    initial_parser.add_argument('--num_workers', 
                                type=int, 
                                default=8,
                                help='Num of workers'
                                )
    initial_parser.add_argument('--lr', 
                                type=float, 
                                default=3e-4,
                                help='Learning rate (required), recommend 1e-5'
                                )
    initial_parser.add_argument('--wd', 
                                type=float, 
                                default=1e-5,
                                help='Weight decay, recommend 1e-5'
                                )
    initial_parser.add_argument('--cosine_eta_min', 
                                type=float, 
                                default=1e-6,
                                help='min lr in cosine scheduler, recommend 1e-6'
                                )
    initial_parser.add_argument('--cosine_T_max', 
                                type=int, 
                                default=50,
                                help='max T in cosine scheduler, recommend the number which half of the epochs)'
                                )
    initial_parser.add_argument('--early_stop_patience', 
                                type=int, 
                                default=60,
                                help='Early Stop after epochs, recommend 60, to cover the half of epochs'
                                )
    initial_parser.add_argument('--scheduler_type', 
                                type=str, 
                                default="cosine",
                                help='Type of scheduler'
                                )
    initial_parser.add_argument('--save_max', 
                                type=int, 
                                default=2,
                                help='Max saved number'
                                )
    initial_parser.add_argument('--interval', 
                                type=int, 
                                default=1,
                                help='Early Stop after epochs'
                                )
    initial_parser.add_argument('--commit', 
                                type=str, 
                                default="Training",
                                help='commit'
                                )
    initial_parser.add_argument('--datasets', 
                                type=str, 
                                default="BraTS2019",
                                help='datasets name, BraTS2019 or BraTS2021'
                                )
    
    #*** æµ‹è¯•å‚æ•° ***#
    initial_parser.add_argument('--data_split', 
                                action='store_true',
                                help='Flag of splitting dataset'
                                )
    initial_parser.add_argument('--local', 
                                action='store_true',
                                help='Flag of partial dataset'
                                )
    initial_parser.add_argument('--train_length', 
                                type=int, 
                                default=70,
                                help='Num of partial train dataset'
                                )
    initial_parser.add_argument('--val_length', 
                                type=int, 
                                default=10,
                                help='Num of partial val dataset'
                                )
    initial_parser.add_argument('--test_length', 
                                type=int, 
                                default=10,
                                help='Num of partial test dataset'
                                )
    initial_parser.add_argument('--slb', 
                                action='store_true',
                                help='Flag of SwanLab'
                                )
    initial_parser.add_argument('--tb', 
                                action='store_true',
                                help='Flag of TensorBoard'
                                )
    initial_args = initial_parser.parse_args()
    
    end_time = time.time()
    logger.info(f"åŠ è½½é…ç½®æ–‡ä»¶è€—æ—¶: {end_time - start_time:.2f} s")
    
    # ä¼ é€’åˆå¹¶åçš„å‚æ•°
    main(args=initial_args)